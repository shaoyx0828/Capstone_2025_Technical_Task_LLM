import torch
import torch.nn as nn
from transformers import AutoConfig, AutoModelForSequenceClassification
from peft import PeftModel, PeftConfig

class QwenWithMLPForSequenceClassification(nn.Module):
    def __init__(self, base_model_name, lora_model_path, hidden_dim=512, dropout=0.1):
        super().__init__()

        # åŠ è½½ LoRA adapter é…ç½®
        peft_config = PeftConfig.from_pretrained(lora_model_path)

        # é…ç½®è®¾ç½®
        config = AutoConfig.from_pretrained(
            base_model_name,
            num_labels=3,  # ğŸ”¥ ä¸€å®šè¦å’Œè®­ç»ƒæ—¶ LoRA ä¿æŒä¸€è‡´
            trust_remote_code=True
        )

        # åŠ è½½å¸¦åˆ†ç±»å¤´çš„åŸºç¡€æ¨¡å‹ï¼ˆæ”¯æŒ labelsï¼‰
        base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name,
            config=config,
            trust_remote_code=True
        )

        # åº”ç”¨ LoRA adapter
        self.model = PeftModel.from_pretrained(base_model, lora_model_path)

        # MLP åˆ†ç±»å¤´
        self.dropout = nn.Dropout(dropout)
        self.mlp_head = nn.Sequential(
            nn.Linear(config.num_labels, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, config.num_labels)
        )

        # è®¾ç½® pad_token_idï¼Œé¿å… Qwen æŠ¥é”™
        self.model.config.pad_token_id = self.model.config.eos_token_id

    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        outputs = self.model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        return_dict=True
    )

    # å–åˆ†ç±» logitsï¼ˆ[batch_size, num_labels]ï¼‰
        x = outputs.logits  # [B, num_labels]ï¼Œæ¯”å¦‚ [16, 3]
        x = self.dropout(x)  # å¯åŠ å¯ä¸åŠ 
        logits = self.mlp_head(x)  # è¿›å…¥ä½ è‡ªå®šä¹‰çš„ MLP åˆ†ç±»å¤´

        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(logits, labels)

        return {
            "loss": loss,
            "logits": logits
        }